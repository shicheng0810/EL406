# EL406 Clog Detection System

## Introduction

The EL406 Clog Detection System is an in-house solution designed to identify syringe clogs in the Agilent BioTek EL406 microplate washer/dispenser. Developed as a collaboration between Carnegie Mellon University and Generate Biomedicines, this system uses a Raspberry Pi with a camera module and custom software to monitor liquid dispensing streams in real time. By automatically detecting anomalies (like an absence of fluid stream from a dispense nozzle), the system provides rapid and reliable clog identification, thereby enhancing experimental robustness and reducing instrument downtime. In essence, this project integrates computer vision and hardware triggers to catch clogged dispense nozzles **as they happen**, preventing failed experiments and maintenance headaches.

## System Requirements

Before setting up the clog detection system, ensure you have the following hardware and software:

* **Biotek EL406 Washer/Dispenser** – The liquid handling instrument to be monitored (integration is non-intrusive to the EL406).
* **Raspberry Pi 5 Model B** – Recommended for performance and compatibility (the Pi 5’s improved CPU/GPU handles real-time video processing well). A Raspberry Pi 4 may work but the Pi 5 is optimal.
* **Camera Module** – A Raspberry Pi-compatible camera (e.g. the official HQ camera or an `Autocam` module) mounted overhead to view the EL406’s dispense head and streams.
* **Mechanical Micro-Switch** – A switch sensor to detect the dispensing head’s position or movement. This is used to know when priming/dispensing starts and stops. A small lever or button switch that the EL406 head can press is ideal.
* **Power Supply** – Adequate 5V power supply for the Raspberry Pi (Pi 5 typically requires 5V⎓5A).
* **Raspberry Pi OS (64-bit)** – Use the latest Raspberry Pi OS (64-bit) on the Pi. Ensure the camera interface is enabled (via `raspi-config` if needed) and that **libcamera** support is working (this system uses `libcamera-vid` for video capture).
* **Python 3.9+** – The code runs on Python 3.9 or higher. It is recommended to use a Python 3 virtual environment on the Pi.
* **Python Dependencies** – Key libraries include OpenCV for image processing, NumPy, `gpiod` for GPIO handling, `pupil_apriltag` for fiducial marker detection, and others. All required Python packages are listed in `requirements.txt` and will be installed via pip (see Installation). These require an internet connection on the Pi to download.

**Note:** The EL406 is normally controlled by BioTek’s LHC software on a Windows PC. This detection system runs in parallel on the Raspberry Pi and does not interfere with the EL406’s operation. You will use the EL406 as usual (via the PC software) while the Pi monitors the dispensing.

## Installation

### Hardware Setup

Setting up the hardware involves mounting the camera and the switch, and connecting them to the Raspberry Pi:

1. **Position the Camera:** Power off the EL406 and the Raspberry Pi while installing hardware. Attach the camera to the Pi’s camera port and mount it above the EL406 in a stable position (for example, on a frame or stand). The camera should have a clear **top-down view of the EL406’s dispensing head and all nozzle streams**. Adjust focus if needed so that liquid streams are visible and in frame.
2. **Install the Mechanical Switch:** Mount the microswitch such that the moving dispense head of the EL406 will engage it at a specific point in the cycle. A common placement is on the EL406 frame where the dispense head or arm will press the switch when it’s in the “home” or priming position. Ensure the switch is firmly in place and can be repeatedly triggered by the head’s motion.
3. **Connect to the Raspberry Pi:** Wire the switch to the Pi’s GPIO pins. By default, the system is configured to use **BCM GPIO 26** and **BCM GPIO 23** as the switch inputs. It’s recommended to use a switch with both normally-open (NO) and normally-closed (NC) contacts so that both pins can be utilized for dual signals (see **Switch Setup** below). Connect the common terminal of the switch to a ground pin on the Pi, the NO terminal to GPIO 23, and the NC terminal to GPIO 26. This configuration lets the software detect both the press and release of the switch. (If you use two separate switches for different positions, connect one to each GPIO in a similar ground-to-pin manner.)
4. **Secure Power and Cables:** Re-connect power to the Raspberry Pi (ensure a stable 5V supply, especially for Pi 5). Connect the Raspberry Pi to the network (for installing packages) or attach a keyboard/monitor for setup. Double-check that the camera’s ribbon cable and the switch’s wires are securely attached. You can now power the Raspberry Pi back on, as well as the EL406 when ready.

### Software Setup (Python Virtual Environment)

With hardware in place, set up the software environment on the Raspberry Pi 5:

1. **Prepare the OS:** Boot up the Raspberry Pi with Raspberry Pi OS (64-bit). Ensure the system is up-to-date by running `sudo apt update && sudo apt upgrade`. You may also need to enable the camera interface (using `sudo raspi-config`, enable **Interface Options > Camera** if using legacy camera stack, or ensure the Pi Camera works with `libcamera-hello` for the modern stack). Also install `python3-venv` if not already present (`sudo apt install -y python3-venv`).
2. **Create a Virtual Environment:** It’s best to run the detection code in an isolated Python environment. Use Python 3 to create a virtual environment:

   ```bash
   python3 -m venv ~/el406-venv
   source ~/el406-venv/bin/activate
   ```

   This creates and activates a virtual environment named “el406-venv” in your home directory. (You can also create it in the project folder if preferred.) Once activated, your shell prompt should change to indicate the venv is active.
3. **Install OpenCV Dependencies (if needed):** The `opencv-python` package will be installed via pip, but on Raspberry Pi you might need some system libraries. If you encounter errors installing or importing OpenCV later, ensure that `libatlas-base-dev`, `libjpeg-dev`, and other common dependencies are installed (`sudo apt install -y libatlas-base-dev libjpeg-dev`). In most cases, pip will install a pre-built OpenCV wheel for arm64, so this step is just a precaution.

### Cloning the Repository and Installing Dependencies

With the virtual environment ready, download the clog detection code and install its Python dependencies:

1. **Clone the GitHub Repository:** Retrieve the project code from GitHub. In the terminal, run:

   ```bash
   git clone https://github.com/shicheng0810/EL406.git
   cd EL406
   ```

   This will create an `EL406` directory (or use the name of the repo) and change into it. The repository contains the Python scripts and a `requirements.txt` file.
2. **Install Python Requirements:** Inside the repository directory, ensure your virtual environment is activated (`source ~/el406-venv/bin/activate` if not already). Then install all required Python packages:

   ```bash
   pip install -r requirements.txt
   ```

   This will download and install packages like OpenCV, NumPy, `gpiod` (for GPIO via libgpiod), `pupil_apriltags`, scikit-learn, etc. Internet access is required for this step. The requirements are pinned to specific versions known to work on Raspberry Pi 5. **Note:** If the `gpiod` installation fails, you might need to install the underlying system library with `sudo apt install -y libgpiod-dev` and then retry the pip install.
3. **(Optional) Verify Installation:** After pip completes, you can verify that the major packages installed correctly. For example, run `python -c "import cv2, numpy, gpiod; print('OK')"` and ensure it prints “OK” without errors. If you run into an error, address it (for instance, an OpenCV import error might indicate missing dependencies as noted above).

At this point, the hardware is connected and the software environment is set up. You’re ready to configure the switch logic and then run the clog detection system.

## Switch Setup

The mechanical switch serves as a trigger for the system to distinguish **priming vs. dispensing** operations and to start/stop video recording at the correct moments. Proper configuration and wiring of this switch are crucial:

* **Wiring:** As mentioned in Installation, by default the software expects two GPIO input lines: one for the main switch signal (`GPIO26`) and one for a secondary mode signal (`GPIO23`). If you used a single dual-contact (SPDT) switch, ensure one contact is wired to each of these pins (with common ground). This setup allows the software to detect both the **press** and **release** of the switch. The press/release cycle is used to mark the transition between priming and dispensing modes. If you use two separate switches (less common), wire one to each pin (both sharing ground).

* **Software Configuration:** The default GPIO pins (23 and 26) are configured in the code’s `DEFAULTS` settings. If you need to use different pins, you can modify these in `multi_segment_recorder.py` (look for the `DEFAULTS` dictionary). Keep in mind these are Broadcom (BCM) pin numbers. Also, the code uses internal pull-ups on these lines, so you should wire the switch to ground (so that pressing the switch pulls the input to ground, and releasing it lets it float high).

* **Switch Positioning:** The switch should be installed such that **when the EL406’s dispense head is in a known position, the switch is pressed**. For example, you might position it so that when the head is at the priming trough or at the top of its travel, it depresses the switch. In operation, the moment the head moves away from that position (releasing the switch) will signal the system to start recording video. When the head returns and presses the switch again, the system will stop recording. This *“release-to-start, press-to-stop”* logic ensures video is captured exactly during the priming or dispensing action. (The program will initially display **“Ready – release to start recording”** to indicate it’s waiting for the switch to be released from its pressed state to begin.)

* **Testing the Switch:** It’s a good idea to test the switch input before running full detection. You can use the `gpiod` tools or a simple Python snippet to monitor the GPIO values. Ensure that you see the value toggle when the switch is manually pressed and released. If the program doesn’t seem to respond to the EL406 actions, double-check that the correct pins are used and that the logic isn’t inverted (if needed, you could swap NO/NC wiring). Once the switch reliably reflects the head movement, you’re ready to proceed.

## Running Recordings for Priming and Dispensing

One of the first uses of the system is to **record video segments** of the EL406 performing its priming and dispensing operations. These videos can be used for calibration and to verify the detection pipeline. The script `multi_segment_recorder.py` automates the recording and tagging of these segments. Below is a step-by-step guide to capture the four key scenarios: *Row A Priming, Row B Priming, Row A Dispensing,* and *Row B Dispensing*.

1. **Prepare the EL406 for Operation:** Ensure the EL406 is loaded with liquid and is connected to its control software on the PC. Decide which operation you will perform first (for example, *Row A Priming*). Typically, Row A and Row B refer to two dispensing syringe sets or manifolds in the EL406 (e.g., front and back rows of dispense nozzles). You will run one operation at a time and let the Pi record it.
2. **Start the Recording Script:** On the Raspberry Pi (with the virtual environment activated and in the project directory), run:

   ```bash
   python multi_segment_recorder.py
   ```

   The script will initialize and print a message such as “**Ready – release to start recording**”. This means it’s **waiting for the switch trigger**. At this point, the EL406’s head should be in the position that keeps the switch pressed (typically the home position).
3. **Perform the Operation (e.g., Priming Row A):** Using the EL406’s control software (BioTek LHC or similar), initiate the chosen operation. For example, send the command to prime Row A. The EL406 dispense head will move and **release the switch**, at which moment the script detects the change and **begins recording** automatically. You should see in the Pi console output that recording has started (it will log a message for segment start). The camera will record the entire priming process. When the priming completes, the dispense head usually returns to its initial position, pressing the switch again – the script will detect that and **stop the recording**. It will log that the segment has stopped and will then proceed to save and analyze the captured video.
4. **Wait for Analysis:** After the segment is recorded, `multi_segment_recorder.py` will rename the raw video file and spawn an analysis in the background. The video file will be saved in a `tmp/` directory under the project (by default) and named with a suffix like `_primming.mjpeg` or `_dispensing.mjpeg` depending on the operation type. For example, after a Row A priming run, you might get a file `segment_1_primming.mjpeg` (the misspelling “primming” is in the code). The script determines the type automatically: since this was a priming operation, it will label it accordingly. Simultaneously, it runs the clog detection analysis on that video. Once analysis is done, the console will output a summary of results for that segment. For instance, it might report something like *“Segment 1 (priming) → {'A': \[True, True, True, True, True, True, True, True]}”*, indicating the status of each nozzle (here, a list of booleans for 8 nozzles – this is just an example format). Typically, a `True` might mean a successful flow and a `False` could indicate a clog, depending on how the results dictionary is structured. In any case, **pay attention to any indication of a clog** in the output.
5. **Repeat for Other Operations:** Now you can repeat the above process for the other scenarios. **For Row B Priming:** re-run the script (`python multi_segment_recorder.py` again) and then initiate a priming of Row B on the EL406. The system will record and analyze it, saving e.g. `segment_1_primming.mjpeg` (or `segment_2_primming.mjpeg` if you haven’t cleared the `tmp` folder) for Row B. Next, do the same for dispensing operations. For **Row A Dispensing:** run the script and then dispense liquid using Row A (for example, dispense into a microplate using only the Row A nozzles). After that, do **Row B Dispensing** in the same manner. Each run of the script will handle one segment (the script will exit after each segment’s recording/analysis is done). You should end up with videos and results for all four cases.
6. **Review the Videos (Optional):** All captured segments are stored as MJPEG files in the `tmp` directory (unless you changed the path in code). You can review these videos (e.g., open them with VLC or convert to MP4 using FFmpeg) to visually confirm the presence or absence of liquid streams for each nozzle. This can be useful to validate that the detection algorithm’s output matches what you see (e.g., if it flagged a clog on nozzle 5, check if nozzle 5’s stream was indeed missing or weak in the video). These videos can also serve as calibration references.

By following the above, you have recorded sample priming and dispensing events for both rows. Next, you will use one or more of these recordings to calibrate the system’s perspective transformation (homography) and possibly tune detection thresholds.

## Calibration

Before fully running the clog detection in real-time, it’s important to calibrate the system so that it knows the exact positions of each dispense nozzle in the camera’s view. This is accomplished via a homography calibration using the `calibration.py` script. The calibration aligns the camera’s perspective with the EL406’s layout (so that the streams line up with defined regions for each nozzle) and stores this information for the detection algorithm. It also can help set initial threshold parameters for detection. Perform calibration as follows:

&#x20;*Figure: Homography Calibration UI – The interface for calibrating perspective. Red circles (control points) are dragged to mark the corners corresponding to the nozzle array for Section A or B. The bottom preview (Section A shown) displays the transformed view with red and white boxes delineating each nozzle’s region.*

1. **Obtain a Calibration Video:** You will need a recorded video of a dispensing cycle to calibrate. You may use one of the videos recorded in the previous step (for example, a priming video where all nozzles fired, or a dispensing video). By default, `calibration.py` looks for a file named `output.mjpeg` in the project directory. If your recorded video has a different name, you can either rename the desired file to `output.mjpeg` or modify the script to point to it. *Tip:* An ideal calibration video is one where liquid is dispensed from all nozzles (so their streams are visible) and where two AprilTag fiducial markers are in view if you’ve placed them (the system can use AprilTags if available). If the system is configured with AprilTags (not mandatory), ensure they are present in the video frame for automatic detection of reference points.
2. **Run the Calibration Script:** Launch the calibration tool on the Raspberry Pi (you will need a display for the GUI or use X forwarding/VNC). Run:

   ```bash
   python calibration.py
   ```

   The script will process the specified video to detect initial features and then open a GUI window titled “Homography Calibration”. In this interface, you will calibrate **Section A** and **Section B** one at a time. Section A and B correspond to the two sets of dispense nozzles (for example, the front row vs. back row of the EL406 dispense head). If your video contained activity for both sections, the tool will step through each; otherwise, you may calibrate them individually using separate videos.
3. **Adjust Calibration Points:** In the Homography Calibration window, you’ll see a black-and-white thresholded panorama of the dispensing area, with some red points overlaid (these are the calibration control points). There is a dropdown at the top to select Section A or B. Select a section (start with A) to adjust its calibration. The red circles represent key points (typically the four corners defining the rectangular region that covers all 8 nozzles of that section in the camera view). You can drag each red point with the mouse to align with the corresponding corner of the section’s nozzle array in the image. For example, drag the points so that they coincide with the extreme corners of where the streams for that section appear. As you move the points, the bottom preview image for that section updates, showing the result of the homography transform. The goal is to have the red-outlined boxes in the preview neatly encompass each nozzle’s stream. In the bottom preview, you will see 8 red rectangles – one for each nozzle’s area – and inside each a smaller white rectangle (which is the region used to compute the “white pixel” ratio for clog detection). Adjust the points until these rectangles align well with where the fluid streams would be for that section.
4. **Save the Calibration:** Once you are satisfied with the alignment for Section A, you can click the **Save** button. The calibration interface saves the homography points to `calibration_data.json` for that section. If the tool then prompts or automatically opens Section B (or if you select Section B from the dropdown), repeat the same process for Section B. Adjust its red corner points so that the preview’s boxes align with the Section B nozzles, then Save for Section B as well. All the homography coordinates will be stored in the JSON file. (If you re-run calibration.py later, it will load the existing points from the JSON, and you can fine-tune if necessary.)
5. **Verify and Exit:** After saving, you can close the calibration window. The homography calibration is now complete. This means the system knows how to warp the camera image to a top-down view where each nozzle corresponds to a fixed column region. You typically only need to do this once initially, and then periodically if the camera or EL406 has been moved (the maintenance section of the user guide suggests recalibrating monthly). If you had placed AprilTag fiducials and the system detected them (fiducial coordinates are logged to the JSON as well), those might have auto-populated the initial point positions, making your job easier – but manual adjustment ensures precision.

With homography calibrated (and threshold parameters possibly set in the JSON as well), the detection algorithm is ready to run with accurate geometrical mapping.

*(Advanced calibration note: There is also a `calibration_threshold.json` and an optional script `calibration_frame+position.py` in the repository. These tools can be used to fine-tune the **white pixel ratio threshold** and the **initial frame offset** used in detection by comparing results against ground truth. For most users, this is not required initially. The default threshold and start frame (e.g. 3% white pixel ratio, starting at frame 30) work well for typical use【31†】. If you notice false positives/negatives, you could use these tools to adjust the sensitivity.)*

## Detection Pipeline Usage

After calibration, the system is ready to perform real-time clog detection. The core detection logic is implemented in the `multi_segment_recorder.py` script (which you used to record segments) and the associated analysis functions in `detect_clogs.py` and `detect_clogs_plate.py`. In normal operation, you will launch the detection script *before* the EL406 runs its dispensing protocol, and the system will automatically capture and analyze each priming or dispensing event. Here’s what to expect and how to use it in practice:

1. **Launch the Detection Program:** Activate your Python virtual environment if not already, and run:

   ```bash
   python multi_segment_recorder.py
   ```

   (Alternatively, you can set this script to run on boot or via a supervisor if you want the Pi to always be monitoring the EL406.) The script will output the “Ready” message indicating it’s waiting on the switch as described earlier. Now, carry out your experiment or protocol on the EL406 normally (e.g., start the plate washing/dispensing routine via the PC software). The detection program will **run continuously until a priming or dispensing cycle is detected**, record that segment, analyze it, then **exit** (by design) after the cycle. If you have multiple cycles in one experiment (e.g., priming followed by dispensing, or multiple dispenses), you will need to restart the script for each cycle unless you modify it for continuous looping. In many cases, priming and dispensing happen only once per experiment, so a single run suffices.
2. **Automatic Mode Detection:** The system uses the switch signals and internal logic to determine whether a given segment is a **priming** or **dispensing** operation. You do *not* need to specify this – it’s detected automatically. Specifically, the code monitors the secondary GPIO (the “mode pin”) during a recording. If a certain switch pattern is observed (for instance, a particular rise on the mode pin), the segment is labeled as dispensing; otherwise, it’s labeled as priming. This means the software will correctly classify the type of operation in real time, and then apply the appropriate analysis pipeline for that mode. For priming, the algorithm is tuned to detect anomalies during the initial flush of liquid (e.g., slower or no flow from a nozzle). For dispensing, the algorithm checks each column of wells for liquid flow (essentially looking for any nozzle that failed to dispense into its well).
3. **Real-Time Analysis:** As the video is being recorded, or immediately after, the detection algorithms process the footage. Under the hood, the system performs a series of steps: it applies the **homography transformation** to get a top-down view, then uses background subtraction and thresholding to isolate the liquid streams (which appear as white regions on a black background). It divides the view into regions corresponding to each nozzle and calculates the **white pixel ratio** in those regions over time. Using a rolling accumulation of frames (processing roughly every 10th frame to reduce noise), it determines if a particular nozzle’s region consistently lacks the expected amount of liquid. If so, that nozzle is flagged as having a clog or dispense failure. This frame-by-frame detection algorithm is optimized for real-time use on the Pi, leveraging the Pi 5’s CPU and limited GPU acceleration (via OpenCV).
4. **Output and Results:** The detection outcome for each segment is printed to the console once analysis completes. It will clearly report if any clogs were detected. For example, the output might indicate something like *“Nozzle B3 CLOGGED”* or provide a list of booleans per nozzle as mentioned earlier. In priming mode, a clog detection would mean that a particular syringe did not successfully prime (no fluid observed). In dispensing mode, a clog detection corresponds to a well (or column of wells) that received little or no liquid from its nozzle. The system also saves the analyzed video and intermediate results for review. The video files (with `_primming.mjpeg` or `_dispensing.mjpeg` suffixes) are in the `tmp` directory (you can change the save location in the code configuration). You can archive these or examine them later. If a clog is detected, you should take appropriate action (e.g., pause the experiment, alert a technician, or re-run the dispense for that well after fixing the issue).
5. **(Optional) Notifications:** While not required, the system can be extended to send notifications. In the user guide it’s noted that upon detecting a clog, a Slack message could be sent. In this codebase, an automated notification feature may not be fully implemented (no direct Slack integration is present in the scripts). However, you could integrate such a feature by adding a few lines in the detection script to call a webhook or send an email when a clog is found. This would allow you to get immediate alerts remotely when running long experiments.

In summary, once everything is calibrated, using the detection system is as simple as running the Python script and then operating the EL406 normally. The Pi will take care of recording and analyzing each dispensing cycle, and will inform you of any problems in near real-time. This adds a layer of oversight to the EL406, catching issues that might otherwise go unnoticed until after an assay or experiment is complete.

## Troubleshooting

Despite careful setup, you might encounter some challenges. Here are some common issues and their solutions:

* **No switch response (system never starts/stops recording):** If the program stays at “Ready – release to start” or doesn’t react when it should, the switch signal isn’t reaching the software. Double-check the GPIO wiring – ensure the correct pins are used and connected to ground through the switch. Make sure the switch is being physically triggered by the EL406’s movement. You can use the `gpiod` command-line tools (`gpioget` on the chip lines) to see if the Pi registers the pin changing. Also verify that the `gpiod` Python library is installed and the program is running with appropriate privileges (running as root or a user in the `gpio` group). Installing or reinstalling the `libgpiod2` system package might help if there’s a library issue. In short, **check the hardware connection and the `gpiod` setup**.
* **False positives (clogs reported when everything dispensed fine):** The detection threshold might be too sensitive. Each nozzle’s flow is judged by a white pixel ratio; if lighting or reflections are off, it could mis-classify a good flow as insufficient. To fix this, you can adjust the **ratio threshold or the initial frame index** used by the algorithm. For example, you might increase the `best_threshold` value in `calibration_threshold.json` (e.g., from 0.03 to 0.05) so that small amounts of noise aren’t counted as lack of liquid. You could also adjust the `start_frame` so that analysis begins a bit later, giving time for flows to stabilize (for instance, start at frame 10 instead of frame 1, to ignore the very beginning where there might be a delay). Re-run calibration or directly edit the JSON config, then test again with known good runs to validate that false positives are eliminated. Proper lighting and camera exposure also help reduce false detections (ensure the streams appear clearly distinct from background).
* **Frame skipping or video performance issues:** If you notice that the recorded video has jumps or the analysis prints warnings about dropped frames, the Pi might be struggling to keep up. This could happen if the resolution is set too high or if the Pi is under heavy load. The default recording resolution is 760×540 at 30 FPS, which should be fine on Pi 5. However, if needed, you can **lower the resolution or frame rate** to reduce load. Edit the `DEFAULTS` in `multi_segment_recorder.py` – for example, try 640×480 or reduce FPS to 24. Also ensure the Pi’s GPU has sufficient memory (in `raspi-config`, Memory Split). Using a faster storage (SD card or USB drive) for writing videos can help if I/O is the bottleneck.
* **Cannot connect via SSH or lost connection:** This isn’t specific to the clog detection software, but since you might be controlling the Pi remotely, SSH issues can be a roadblock. If SSH fails, you might need to use a direct monitor/keyboard or try resetting the network. Ensure the Pi is on the same network and the IP is correct. In some cases, re-generating SSH keys or using username/password authentication (instead of key-based) can resolve login problems. If you consistently face SSH drops during long runs, consider using a tool like `tmux` or `screen` on the Pi so the program stays running even if your session disconnects.

If other issues arise, consult the user guide or project wiki (if available) for more tips. Regular maintenance can prevent problems – for example, **check the alignment of the camera and the condition of the switch weekly**, and re-run the calibration monthly or after any significant changes to the setup. Keeping the software up to date (pull the latest code from Git if improvements are made) is also recommended.

With the EL406 Clog Detection System properly installed and calibrated, you should gain an extra layer of confidence in your high-throughput dispensing experiments. The system will diligently watch each priming and dispensing cycle, and alert you to any clogs in real time, helping ensure that your assays get the volumes they need in every well. Good luck with your setup, and happy clog-free dispensing!
